---
title: "IML 2023: Predicting Saturation Vapour Pressure From Molecular Properties"
author: "Helmi Karesti, Janne Penttala, Maija Säisä"
output: pdf_document
date: "2023-12-10"
---

# Introduction

The objective of our project was to build a machine learning model to predict saturation vapour pressure based on some interpretable features. We had given datasets to help us build our model. We started by getting familiar with the data through data exploration and preprocessing. After we had verified that our data was clean, we used it to select our model. Finally after model selection we applied few different methods to try optimize the performance and accuracy of our model. This report contains a more in-depth explanation of our methods regarding the steps of building our model.

# Data analysis

## Preprocessing

As a first step to preprocessing we made sure that our data is valid. In other words we wanted to ensure there are no missing nor mismatched values for any of the variables. This was quite straightforward as Kaggle provided us variable-specific graphs for each variable, meaning we could verify the validity of both the training and test data with the information from Kaggle. 

After validating the data there were a few minor changes we decided to make to further prepare our data. First of all we decided to remove the 'Id' column from both data sets, as this would not be needed in predicting our target variable. The other thing we did was apply one-hot encoding for a variable called 'parentspecies', as this was the only categorial variable. A small minor fix that we needed to make after applying one-hot encoding was to create an extra column for the test set to match our training set. 

Lastly we wanted to ensure data consistency of both training and test data. To do this we simply compared the two data sets to each other and made sure that both of them have the same columns in the same order.

## Data exploration

The data used in the project is based on GeckoQ dataset. Our first step with the project was to get familiar with this dataset and get a preliminary understanding of the features it has. Our plan was to first use material that was already provided by the project descriptions and then continue with some more data exploration by handling the data ourselves. We first got familiar with the features and their descriptions. Understanding the features perfectly isn't necessarily important in this context, however it is useful to have a general understanding of how they might affect our target variable.

To get a better understanding of our data, we plotted each variable against our target variable. We did this with and without scaling. As a result we got a scatter plot, histogram and boxplot of each variable. Analyzing these plots helped us understand the relationship of each variable and the target variable better. This would turn out useful in selecting the features for our model.

# Methods

## Model selection

For selecting our model we first compared different models and their results in predicting the target variable. We evaluated the performance of the models based on their R2 score. The models we considered were Dummy Model, OLS Linear Regression, Random Forest, Support Vector Regressor, and Lasso Regression. After testing these five different models with and without scaling, the best results were obtained by Support Vector Regression model.

We also applied cross-validation in order to verify further our choice of model. We did this using k-fold cross-validation (with k set to 5). The performance was once again evaluated by the models R2 score. As a result Support Vector Regression model had once again the best performance.

## Optimizing the model

To optimize our model to perform better we tried removing outliers and selecting less features. The best results were obtained by dropping features that have minimal impact to our target variable.

# Results


# Discussion

